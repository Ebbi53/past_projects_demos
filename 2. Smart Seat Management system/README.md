# Smart Seat Management System

**Project Description:** My application uses video footage to identify seats, map those seats to the pre-defined seating plan and get their occupancy status (available, occupied, reserved) . The ‘Reserve’ status is kept as a standard of 30 minutes after which it is marked as available for other users. The time counter for reservation time is reset only when the previous user returns. The seating plan reflects the current status on the UI in real-time. Users can choose from the given available seats and get the directions to that seat from the library entrance. Further technical details is as follows:

#### Machine Learning (CV) component:

* Used **[YOLOv3](https://pjreddie.com/darknet/yolo/)** model via [Open CV](https://opencv.org/), pre-trained on **[COCO dataset](https://cocodataset.org/#home)** for object detection
* During the video processing stage of the application, when the image of each frame is passed through the network, the bounding boxes are filtered to human and chair classes only. Each tracked seat has a predesignated region in the camera's view with an associated image for background subtraction. If a human bounding box is detected with a significant overlap with a seat's region, then that seat is marked as occupied. Otherwise, the system would perform background subtraction on the region to check for any objects that could mark the seat as reserved. In case of no such objects, the seat would be marked as available.
* In order to account for small fluctuations in state such as a person walking past a seat or a brief failure of the detection model, a build-up/decay system is being used where the formal labelling of the seat's state presented by the system outside of its internal logging and to the frontend user interface only occurs once the new state is held for at least **50 frames**.
* This unique approach of having designated seat regions, performing background subtraction on those regions, and using the build-up/decay system to account for variations increased the accuracy of the system by **1.5 times**.
* **Testing** - A control environment was manually setup to record a video with the events that were pre-planned and fixed so that the ground-truth labels of the objects in the video frames can easily be calculated. These labels were then compared against the labels generated by the system and the discrepancies visualized (as shown by Graphs. 1 to 3). The __green__ area of the graph represents __available__, __red__ represents __taken__, and __yellow__ represents __reserved__. The __upper half__ of the graphs visualizes the labels detected by the __system__ while the __below half__ visualizes the __ground-truth labels observed by me__.

Graph 1: ![Testing 1](https://github.com/Ebbi53/past_projects_demos/blob/master/%202.%20Smart%20Seat%20Management%20system/cv-seat-detection/data/bar_graph_seat0.png)
Graph 2: ![Testing 2](https://github.com/Ebbi53/past_projects_demos/blob/master/%202.%20Smart%20Seat%20Management%20system/cv-seat-detection/data/bar_graph_seat1.png)
Graph 3: ![Testing 3](https://github.com/Ebbi53/past_projects_demos/blob/master/%202.%20Smart%20Seat%20Management%20system/cv-seat-detection/data/bar_graph_seat2.png)
Graph 4: ![Testing 4](https://github.com/Ebbi53/past_projects_demos/blob/master/%202.%20Smart%20Seat%20Management%20system/cv-seat-detection/data/bar_graph_seat3.png)

* The same control dataset was used to calculate the accuracy of the system. The soft accuracy considering the transition frames turned out to be __90.25%__ while the hard accuracy was __86.37%__.

#### Web application Component:

* **[NodeJS](https://nodejs.org/en/)** is used at the Backend for WebServer
    * A node module called [node-pty](https://github.com/microsoft/node-pty) is used to establish a pseudoterminal to fork Python process and initiate the communication with the ML component.
    * The database is hosted locally using **[MongoDB](https://www.mongodb.com/)** where the data is stored in NoSQL format.
    * **[ExpressJS](https://expressjs.com/)** is used to manage the routes and handle requests.
    * **WebSockets** protocol is used to establis a bi-directional client-server communication using [Socket.io](https://socket.io/)
* For the Frontend UI:
    * Used **[RequireJS](https://requirejs.org/)** to give **Modular structure** to the code and handle the **dependencies** properly by **clearly declaring them** in an entry point of the application (in my case main.js).
    * Used **[BackBone.js](https://backbonejs.org/#)** as an **MVC** (Model, View, Collection) framework to further organise the code into different parts with each part responsible for a specific component of the application.
    * For styling and presentation, used [Bootstrap](https://getbootstrap.com/).

---

### Demo:

*__YouTube__: https://youtu.be/BEROIVwbmGc*

![Screen Recording](https://github.com/Ebbi53/past_projects_demos/blob/master/%202.%20Smart%20Seat%20Management%20system/demo.gif)


**ML outputs:**

![ML output 1](https://github.com/Ebbi53/past_projects_demos/blob/master/%202.%20Smart%20Seat%20Management%20system/ML%20output1.gif)

*YouTube: https://youtu.be/TuEk-_IzVgk*

![ML output 2](https://github.com/Ebbi53/past_projects_demos/blob/master/%202.%20Smart%20Seat%20Management%20system/ML%20output2.gif)

*YouTube: https://youtu.be/PXjMJZObjIg*
